I'll now update the Canvas to showcase **Project #3: The LLM Prompt Injection Tester**.

This version of the dashboard transforms the "Auditor" into a security-focused environment for AI. It features a "Jailbreak Simulator" to show how adversarial prompts attempt to bypass system instructions, a terminal for live red-teaming traces, and security-specific metrics like "Leakage Probability" and "Safety Guardrail Hits."

### Key Highlights for Project #3 (LLM Security Tester):

1. **AI Red-Teaming UI:** The dashboard is themed around security, using a rose/indigo palette to distinguish between "Safety" and "Attacks."
2. **Jailbreak Simulator:** Instead of an accessibility preview, this version simulates the "Prompt vs. Defense" interaction. It demonstrates how your tool intercepts malicious requests before they reach the user.
3. **Inference Audit Trace:** The terminal simulates a real-time security scan of an LLM's inference stream, showing the decision-making process of the "Guardrails."
4. **Security Metrics:** Highlighting concepts like "System Leakage" and "Blocked Attacks" shows hiring managers that you understand the modern AI security landscape (OWASP Top 10 for LLMs).

You've now got the second major project visualised in Canvas. Would you like to keep iterating on this security tool—perhaps adding a "Risk heatmap" visualization—or should we move on to **Project #1 (The Public-First OpenAPI Documentation Portal)**?
